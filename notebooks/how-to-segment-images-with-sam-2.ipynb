{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Segment Images with Segment Anything 2 (SAM2)\n",
        "\n",
        "---\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything-2)\n",
        "\n",
        "Segment Anything Model 2 (SAM 2) is a foundation model designed to address promptable visual segmentation in both images and videos. The model extends its functionality to video by treating images as single-frame videos. Its design, a simple transformer architecture with streaming memory, enables real-time video processing. A model-in-the-loop data engine, which enhances the model and data through user interaction, was built to collect the SA-V dataset, the largest video segmentation dataset to date. SAM 2, trained on this extensive dataset, delivers robust performance across diverse tasks and visual domains.\n",
        "\n",
        "![segment anything model](https://media.roboflow.com/notebooks/examples/segment-anything-model-2-paper.jpg)\n",
        "\n",
        "This notebook is an extension of the official [notebook](https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/image_predictor_example.ipynb) prepared by Meta AI.\n",
        "\n",
        "## Complementary materials\n",
        "\n",
        "---\n",
        "\n",
        "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-images-with-sam-2.ipynb)\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/what-is-segment-anything-2)\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on Segment Anything Model 2 (SAM2).\n",
        "\n",
        "[![SAM2 blogpost](https://media.roboflow.com/notebooks/examples/blog-what-is-sam-2.png)](https://blog.roboflow.com/what-is-segment-anything-2)"
      ],
      "metadata": {
        "id": "l2Es_L1iNX4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "8wkV75Db9tXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "DSffnnWDNhb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA9bQMozM_wg",
        "outputId": "04361001-7dcd-4b77-dfe1-3223cd266b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 30 16:46:46 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
      ],
      "metadata": {
        "id": "H7YQbFlINnGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx8UNmABNkJP",
        "outputId": "6ab275c8-8a25-43b9-e4f3-19dcd16e895e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOME: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install SAM2 and dependencies"
      ],
      "metadata": {
        "id": "yo5LAKqyNzfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "%cd {HOME}/segment-anything-2\n",
        "!pip install -e . -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBrYxp6nNpqk",
        "outputId": "5d9173ff-f130-4636-c2ef-5f05b775fd52"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'segment-anything-2'...\n",
            "remote: Enumerating objects: 1070, done.\u001b[K\n",
            "remote: Total 1070 (delta 0), reused 0 (delta 0), pack-reused 1070 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1070/1070), 128.11 MiB | 15.19 MiB/s, done.\n",
            "Resolving deltas: 100% (381/381), done.\n",
            "/content/segment-anything-2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 76, in resolve\n",
            "    collected = self.factory.collect_root_requirements(root_reqs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 538, in collect_root_requirements\n",
            "    reqs = list(\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 494, in _make_requirements_from_install_req\n",
            "    cand = self._make_base_candidate_from_link(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 210, in _make_base_candidate_from_link\n",
            "    self._editable_candidate_cache[link] = EditableCandidate(\n",
            "                                           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 328, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\n",
            "    self.dist = self._prepare()\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 235, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 338, in _prepare_distribution\n",
            "    return self._factory.preparer.prepare_editable_requirement(self._ireq)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 698, in prepare_editable_requirement\n",
            "    dist = _get_prepared_distribution(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 72, in _get_prepared_distribution\n",
            "    abstract_dist.prepare_distribution_metadata(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/distributions/sdist.py\", line 46, in prepare_distribution_metadata\n",
            "    self._prepare_build_backend(finder)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/distributions/sdist.py\", line 78, in _prepare_build_backend\n",
            "    self.req.build_env.install_requirements(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/build_env.py\", line 218, in install_requirements\n",
            "    self._install_requirements(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/build_env.py\", line 278, in _install_requirements\n",
            "    call_subprocess(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/subprocess.py\", line 151, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1477, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1634, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1644, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1706, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 978, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.11/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1230, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
            "    msg = self.format(record)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 953, in format\n",
            "    return fmt.format(record)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 695, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 645, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.11/traceback.py\", line 124, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/traceback.py\", line 728, in __init__\n",
            "    self.stack = StackSummary._extract_from_extended_frame_gen(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/traceback.py\", line 433, in _extract_from_extended_frame_gen\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.11/traceback.py\", line 318, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/tokenize.py\", line 398, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/tokenize.py\", line 367, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "            ^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/tokenize.py\", line 325, in read_or_stop\n",
            "    return readline()\n",
            "           ^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supervision jupyter_bbox_widget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGv98ypgPA7c",
        "outputId": "3a4e56f9-f2be-4b90-dd50-c6d436d6c3a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/181.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.7/220.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m842.5/842.5 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download SAM2 checkpoints\n",
        "\n",
        "**NOTE:** SAM2 is available in 4 different model sizes ranging from the lightweight \"sam2_hiera_tiny\" (38.9M parameters) to the more powerful \"sam2_hiera_large\" (224.4M parameters)."
      ],
      "metadata": {
        "id": "g3Psmg3sOzIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P {HOME}/checkpoints"
      ],
      "metadata": {
        "id": "Dq_DR0IJN_1H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download example data\n",
        "\n",
        "**NONE:** Let's download few example images. Feel free to use your images or videos."
      ],
      "metadata": {
        "id": "7zPgcOxiQHiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg -P {HOME}/data"
      ],
      "metadata": {
        "id": "_p1zpkKZPvFu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "BHRLQPV4WKd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import base64\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator"
      ],
      "metadata": {
        "id": "vIcNq3IiXufS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "d4ce6f3e-73c2-47c2-c6db-bcfc32275da6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'hydra'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9561bf426160>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msupervision\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msam2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_sam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_sam2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msam2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msam2_image_predictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSAM2ImagePredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msam2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_mask_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSAM2AutomaticMaskGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/segment-anything-2/sam2/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# LICENSE file in the root directory of this source tree.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minitialize_config_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_hydra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalHydra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hydra'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** This code enables mixed-precision computing for faster deep learning. It uses bfloat16 for most calculations and, on newer NVIDIA GPUs, leverages TensorFloat-32 (TF32) for certain operations to further boost performance."
      ],
      "metadata": {
        "id": "svThmVIGZZAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "MZAXCVT0ZWkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NLeXwS2UQU5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = f\"{HOME}/checkpoints/sam2_hiera_large.pt\"\n",
        "CONFIG = \"sam2_hiera_l.yaml\"\n",
        "\n",
        "sam2_model = build_sam2(CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False)"
      ],
      "metadata": {
        "id": "xHvgsf08QRZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automated mask generation\n",
        "\n",
        "Since SAM 2 can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image.\n",
        "\n",
        "The class `SAM2AutomaticMaskGenerator` implements this capability. It works by sampling single-point input prompts in a grid over the image, from each of which SAM can predict multiple masks. Then, masks are filtered for quality and deduplicated using non-maximal suppression. Additional options allow for further improvement of mask quality and quantity, such as running prediction on multiple crops of the image or postprocessing masks to remove small disconnected regions and holes."
      ],
      "metadata": {
        "id": "vIA7L7FoVWqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_generator = SAM2AutomaticMaskGenerator(sam2_model)"
      ],
      "metadata": {
        "id": "72oiBQYvUSws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** OpenCV loads images in BGR format by default, so we convert to RGB for compatibility with the mask generator."
      ],
      "metadata": {
        "id": "wHrJV4HmavcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = f\"{HOME}/data/dog.jpeg\"\n",
        "\n",
        "image_bgr = cv2.imread(IMAGE_PATH)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "sam2_result = mask_generator.generate(image_rgb)"
      ],
      "metadata": {
        "id": "1gjiF_6gV1By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output format\n",
        "\n",
        "`SAM2AutomaticMaskGenerator` returns a `list` of masks, where each mask is a `dict` containing various information about the mask:\n",
        "\n",
        "* `segmentation` - `[np.ndarray]` - the mask with `(W, H)` shape, and `bool` type\n",
        "* `area` - `[int]` - the area of the mask in pixels\n",
        "* `bbox` - `[List[int]]` - the boundary box of the mask in `xywh` format\n",
        "* `predicted_iou` - `[float]` - the model's own prediction for the quality of the mask\n",
        "* `point_coords` - `[List[List[float]]]` - the sampled input point that generated this mask\n",
        "* `stability_score` - `[float]` - an additional measure of mask quality\n",
        "* `crop_box` - `List[int]` - the crop of the image used to generate this mask in `xywh` format"
      ],
      "metadata": {
        "id": "7v1lpyaVWhHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results visualisation"
      ],
      "metadata": {
        "id": "Qyl2Ldc_Wvtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "detections = sv.Detections.from_sam(sam_result=sam2_result)\n",
        "\n",
        "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=[image_bgr, annotated_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        ")"
      ],
      "metadata": {
        "id": "we90u0PSWUeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks = [\n",
        "    mask['segmentation']\n",
        "    for mask\n",
        "    in sorted(sam2_result, key=lambda x: x['area'], reverse=True)\n",
        "]\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=masks[:16],\n",
        "    grid_size=(4, 4),\n",
        "    size=(12, 12)\n",
        ")"
      ],
      "metadata": {
        "id": "wYjjHJH5W09X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced automatic mask generation options\n",
        "\n",
        "There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes.\n",
        "\n",
        "- `points_per_side` `[int or None]` - the number of points to be sampled\n",
        "along one side of the image. The total number of points is\n",
        "`points_per_side**2`. If `None`, `point_grids` must provide explicit\n",
        "point sampling.\n",
        "- `points_per_batch` - `[int]` - sets the number of points run simultaneously\n",
        "by the model. Higher numbers may be faster but use more GPU memory.\n",
        "- `pred_iou_thresh` `[float]` - a filtering threshold in `[0,1]`, using the\n",
        "model's predicted mask quality.\n",
        "- `stability_score_thresh` - `[float]` - a filtering threshold in `[0,1]`, using\n",
        "the stability of the mask under changes to the cutoff used to binarize\n",
        "the model's mask predictions.\n",
        "- `stability_score_offset` - `[float]` - the amount to shift the cutoff when\n",
        "calculated the stability score.\n",
        "- `mask_threshold` - `[float]` - threshold for binarizing the mask logits\n",
        "- `box_nms_thresh` - `[float]` - the box IoU cutoff used by non-maximal\n",
        "suppression to filter duplicate masks.\n",
        "- `crop_n_layers` - `[int]` - if `>0`, mask prediction will be run again on\n",
        "crops of the image. Sets the number of layers to run, where each\n",
        "layer has `2**i_layer` number of image crops.\n",
        "- `crop_nms_thresh` - `[float]` - the box IoU cutoff used by non-maximal\n",
        "suppression to filter duplicate masks between different crops.\n",
        "- `crop_overlap_ratio` - `[float]` - sets the degree to which crops overlap.\n",
        "In the first crop layer, crops will overlap by this fraction of\n",
        "the image length. Later layers with more crops scale down this overlap.\n",
        "- `crop_n_points_downscale_factor` - `[int]` - the number of points-per-side\n",
        "sampled in layer `n` is scaled down by `crop_n_points_downscale_factor**n`.\n",
        "- `point_grids` - `[List[np.ndarray] or None]` - a list over explicit grids\n",
        "of points used for sampling, normalized to `[0,1]`. The nth grid in the\n",
        "list is used in the nth crop layer. Exclusive with points_per_side.\n",
        "- `min_mask_region_area` - `[int]` - if `>0`, postprocessing will be applied\n",
        "to remove disconnected regions and holes in masks with area smaller\n",
        "than `min_mask_region_area`. Requires opencv."
      ],
      "metadata": {
        "id": "syOz7k0iY7Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_generator_2 = SAM2AutomaticMaskGenerator(\n",
        "    model=sam2_model,\n",
        "    points_per_side=64,\n",
        "    points_per_batch=128,\n",
        "    pred_iou_thresh=0.7,\n",
        "    stability_score_thresh=0.92,\n",
        "    stability_score_offset=0.7,\n",
        "    crop_n_layers=1,\n",
        "    box_nms_thresh=0.7,\n",
        ")"
      ],
      "metadata": {
        "id": "c2HjpsmCXdCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sam2_result_2 = mask_generator_2.generate(image_rgb)"
      ],
      "metadata": {
        "id": "mLsIw6PXY95J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "detections = sv.Detections.from_sam(sam_result=sam2_result_2)\n",
        "\n",
        "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=[image_bgr, annotated_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        ")"
      ],
      "metadata": {
        "id": "4gvbXJoaZx8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting with boxes"
      ],
      "metadata": {
        "id": "ijdVA3p0cyJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `SAM2ImagePredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
      ],
      "metadata": {
        "id": "-XnyWFFUc4cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = SAM2ImagePredictor(sam2_model)"
      ],
      "metadata": {
        "id": "gZdNS8fJZ1B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = f\"{HOME}/data/dog-2.jpeg\"\n",
        "\n",
        "image_bgr = cv2.imread(IMAGE_PATH)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)"
      ],
      "metadata": {
        "id": "HZTCMn0MeO7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interactive box prompt"
      ],
      "metadata": {
        "id": "MMzeqSsgejYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        image_bytes = f.read()\n",
        "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
        "    return \"data:image/jpg;base64,\"+encoded"
      ],
      "metadata": {
        "id": "VNWUblWreTW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Execute cell below and use your mouse to **draw bounding box** on the image 👇"
      ],
      "metadata": {
        "id": "A9skH4bceoWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget\n",
        "\n",
        "widget = BBoxWidget()\n",
        "widget.image = encode_image(IMAGE_PATH)\n",
        "widget"
      ],
      "metadata": {
        "id": "gNwusrHEek_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "widget.bboxes"
      ],
      "metadata": {
        "id": "4SFmFWugeqR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** `Sam2ImagePredictor.predict` method takes `np.ndarray` `box` argument in `[x_min, y_min, x_max, y_max]` format."
      ],
      "metadata": {
        "id": "G4vNm8trfN10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_box = [\n",
        "    {'x': 166, 'y': 835, 'width': 99, 'height': 175, 'label': ''},\n",
        "    {'x': 472, 'y': 885, 'width': 168, 'height': 249, 'label': ''},\n",
        "    {'x': 359, 'y': 727, 'width': 27, 'height': 155, 'label': ''},\n",
        "    {'x': 164, 'y': 1044, 'width': 279, 'height': 163, 'label': ''}\n",
        "]\n",
        "\n",
        "boxes = widget.bboxes if widget.bboxes else default_box\n",
        "boxes = np.array([\n",
        "    [\n",
        "        box['x'],\n",
        "        box['y'],\n",
        "        box['x'] + box['width'],\n",
        "        box['y'] + box['height']\n",
        "    ] for box in boxes\n",
        "])"
      ],
      "metadata": {
        "id": "5rMQVWLKeutI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate masks with SAM\n",
        "\n",
        "**NOTE:** Process the image to produce an image embedding by calling `SAM2ImagePredictor.set_image`. `SAM2ImagePredictor` remembers this embedding and will use it for subsequent mask prediction. `SAM2ImagePredictor.predict` takes the following arguments:\n",
        "\n",
        "- `point_coords` - `[np.ndarray or None]` - a `Nx2` array of point prompts to the model. Each point is in `(X,Y)` in pixels.\n",
        "- `point_labels` - `[np.ndarray or None]` - a length `N` array of labels for the\n",
        "point prompts. `1` indicates a foreground point and `0` indicates a\n",
        "background point.\n",
        "- `box` - `[np.ndarray or None]` - a length `4` array given a box prompt to the\n",
        "model, in `[x_min, y_min, x_max, y_max]` format.\n",
        "- `mask_input` - `[np.ndarray]` - a low resolution mask input to the model, typically coming from a previous prediction iteration. Has form `1xHxW`, where\n",
        "for SAM, `H=W=256`.\n",
        "- `multimask_output` - `[bool]` - if true, the model will return three masks.\n",
        "For ambiguous input prompts (such as a single click), this will often\n",
        "produce better masks than a single prediction. If only a single\n",
        "mask is needed, the model's predicted quality score can be used\n",
        "to select the best mask. For non-ambiguous prompts, such as multiple\n",
        "input prompts, `multimask_output=False` can give better results.\n",
        "- `return_logits` - `[bool]` - if true, returns un-thresholded masks logits\n",
        "instead of a binary mask.\n",
        "- `normalize_coords` - `[bool]` - if true, the point coordinates will be normalized to the range `[0,1]` and point_coords is expected to be wrt. image dimensions."
      ],
      "metadata": {
        "id": "k97uhb8itP7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.set_image(image_rgb)\n",
        "\n",
        "masks, scores, logits = predictor.predict(\n",
        "    box=boxes,\n",
        "    multimask_output=False\n",
        ")\n",
        "\n",
        "# With one box as input, predictor returns masks of shape (1, H, W);\n",
        "# with N boxes, it returns (N, 1, H, W).\n",
        "if boxes.shape[0] != 1:\n",
        "    masks = np.squeeze(masks)\n"
      ],
      "metadata": {
        "id": "rbSrtSRIfRex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results visualisation"
      ],
      "metadata": {
        "id": "yL_HThW6fomp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "box_annotator = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "detections = sv.Detections(\n",
        "    xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "    mask=masks.astype(bool)\n",
        ")\n",
        "\n",
        "source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=[source_image, segmented_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        ")"
      ],
      "metadata": {
        "id": "Gb9Retgzfj4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting with points"
      ],
      "metadata": {
        "id": "K2YhLwHiwOnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Execute cell below and use your mouse to **draw points** on the image 👇"
      ],
      "metadata": {
        "id": "ex_EELQswkZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget\n",
        "\n",
        "widget = BBoxWidget()\n",
        "widget.image = encode_image(IMAGE_PATH)\n",
        "widget"
      ],
      "metadata": {
        "id": "LvhZtqzcv1ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "widget.bboxes"
      ],
      "metadata": {
        "id": "biiUhip93tol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_box = [\n",
        "    {'x': 330, 'y': 450, 'width': 0, 'height': 0, 'label': ''},\n",
        "    {'x': 191, 'y': 665, 'width': 0, 'height': 0, 'label': ''},\n",
        "    {'x': 86, 'y': 879, 'width': 0, 'height': 0, 'label': ''},\n",
        "    {'x': 425, 'y': 727, 'width': 0, 'height': 0, 'label': ''}\n",
        "]\n",
        "\n",
        "boxes = widget.bboxes if widget.bboxes else default_box\n",
        "input_point = np.array([\n",
        "    [\n",
        "        box['x'],\n",
        "        box['y']\n",
        "    ] for box in boxes\n",
        "])\n",
        "input_label = np.ones(input_point.shape[0])"
      ],
      "metadata": {
        "id": "xBc0Y_T139lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks, scores, logits = predictor.predict(\n",
        "    point_coords=input_point,\n",
        "    point_labels=input_label,\n",
        "    multimask_output=True,\n",
        ")"
      ],
      "metadata": {
        "id": "c6rS9seW4bbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** With `multimask_output=True` (the default setting), SAM 2 outputs 3 masks, where scores gives the model's own estimation of the quality of these masks. This setting is intended for ambiguous input prompts, and helps the model disambiguate different objects consistent with the prompt. When False, it will return a single mask. For ambiguous prompts such as a single point, it is recommended to use `multimask_output=True` even if only a single mask is desired; the best single mask can be chosen by picking the one with the highest score returned in scores. This will often result in a better mask."
      ],
      "metadata": {
        "id": "iqzBY_T25tux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sv.plot_images_grid(\n",
        "    images=masks,\n",
        "    titles=[f\"score: {score:.2f}\" for score in scores],\n",
        "    grid_size=(1, 3),\n",
        "    size=(12, 12)\n",
        ")"
      ],
      "metadata": {
        "id": "A5tueOO05MwC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}